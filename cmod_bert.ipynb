{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cmod_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MDa7JgotH9VN9wGRr2i-FdXLyjp6h_Qd",
      "authorship_tag": "ABX9TyMbGmlS14Heg6DWVZdiy+I2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h5ng/GNN/blob/master/cmod_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQeR-cToddu0",
        "outputId": "05e954d9-dbe2-46ee-ee1e-2fec74205dee"
      },
      "source": [
        "!pip install fairseq==0.9 transformers==2.9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fairseq==0.9 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: transformers==2.9 in /usr/local/lib/python3.7/dist-packages (2.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (1.8.0+cu101)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (0.29.22)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (4.41.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (1.5.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (1.14.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (1.19.5)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9) (0.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.9) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.9) (0.1.95)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq==0.9) (3.7.4.3)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->fairseq==0.9) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.9) (2.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74Zqw-HtfAbO",
        "outputId": "820d4dcd-27c3-4523-aa23-6082a76c0bf2"
      },
      "source": [
        "import csv\r\n",
        "import os\r\n",
        "import logging\r\n",
        "import argparse\r\n",
        "import random\r\n",
        "from tqdm import tqdm, trange\r\n",
        "import json\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import tor  ch\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "from transformers.tokenization_bert import BertTokenizer\r\n",
        "from transformers.modeling_bert import BertForMaskedLM, BertOnlyMLMHead\r\n",
        "\r\n",
        "from transformers import AdamW\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.append('/content/drive/MyDrive/transformers-data-augmentation/bert_aug')\r\n",
        "from data_processors import get_task_processor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJl0Av8TiXW7"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "BERT_MODEL = 'bert-base-uncased'\r\n",
        "\r\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\r\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\r\n",
        "                    level=logging.INFO)\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0hY52OckZMx"
      },
      "source": [
        "from argparse import Namespace\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "args = {\r\n",
        "    'data_dir': '/content/drive/MyDrive/transformers-data-augmentation/datasets/TREC',\r\n",
        "    'output_dir': 'aug_data',\r\n",
        "    'task_name': 'trec',\r\n",
        "    'max_seq_length': 64,\r\n",
        "    'cache': 'transformers_cache',\r\n",
        "    'train_batch_size': 8,\r\n",
        "    'learning_rate': 4e-5,\r\n",
        "    'num_train_epochs': 10.0,\r\n",
        "    'warmup_proportion': 0.1,\r\n",
        "    'seed': 42,\r\n",
        "    'sample_num': 1,\r\n",
        "    'sample_ratio': 7,\r\n",
        "    'gpu': 0,\r\n",
        "    'temp': 1.0\r\n",
        "}\r\n",
        "args = Namespace(**args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL_--Cgv2GdU"
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, init_ids, input_ids, input_mask, masked_lm_labels):\n",
        "        self.init_ids = init_ids\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.masked_lm_labels = masked_lm_labels\n",
        "        \n",
        "def compute_dev_loss(model, dev_dataloader):\n",
        "    model.eval()\n",
        "    sum_loss = 0.\n",
        "    for step, batch in enumerate(dev_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        _, input_ids, input_mask, masked_ids = batch\n",
        "        inputs = {'input_ids': batch[1],\n",
        "                  'attention_mask': batch[2],\n",
        "                  'masked_lm_labels': batch[3]}\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs[0]\n",
        "        sum_loss += loss.item()\n",
        "    return sum_loss\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, seed=12345):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    # ----\n",
        "    # dupe_factor = 5\n",
        "    masked_lm_prob = 0.15\n",
        "    max_predictions_per_seq = 20\n",
        "    rng = random.Random(seed)\n",
        "\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        modified_example = example.label + \" \" + example.text_a\n",
        "        tokens_a = tokenizer.tokenize(modified_example)\n",
        "        # Account for [CLS] and [SEP] and label with \"- 3\"\n",
        "        if len(tokens_a) > max_seq_length - 3:\n",
        "            tokens_a = tokens_a[0:(max_seq_length - 3)]\n",
        "\n",
        "        # take care of prepending the class label in this code\n",
        "        tokens = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        masked_lm_labels = [-100] * max_seq_length\n",
        "        \n",
        "        cand_indexes = []\n",
        "        for (i, token) in enumerate(tokens):\n",
        "            # making sure that masking of # prepended label is avoided\n",
        "            if token == \"[CLS]\" or token == \"[SEP]\" or (token in label_list and i == 1):\n",
        "                continue\n",
        "            cand_indexes.append(i)\n",
        "        \n",
        "        rng.shuffle(cand_indexes)\n",
        "        len_cand = len(cand_indexes)\n",
        "        output_tokens = list(tokens)\n",
        "        \n",
        "        num_to_predict = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob))))\n",
        "        \n",
        "        masked_lms_pos = []\n",
        "        covered_indexes = set()\n",
        "\n",
        "        for index in cand_indexes:\n",
        "          if len(masked_lms_pos) >= num_to_predict:\n",
        "              break\n",
        "          if index in covered_indexes:\n",
        "              continue\n",
        "          covered_indexes.add(index)\n",
        "\n",
        "          masked_token = None\n",
        "          \n",
        "          # 80% of the time, replace with [MASK]\n",
        "          if rng.random() < 0.8:\n",
        "              masked_token = \"[MASK]\"\n",
        "          else:\n",
        "              # 10% of the time, keep original\n",
        "              if rng.random() < 0.5:\n",
        "                masked_token = tokens[index]\n",
        "              # 10% of the time, replace with random word\n",
        "              else:\n",
        "                masked_token = tokens[cand_indexes[rng.randint(0, len_cand - 1)]]\n",
        "\n",
        "          masked_lm_labels[index] = tokenizer.convert_tokens_to_ids([tokens[index]])[0]\n",
        "          output_tokens[index] = masked_token\n",
        "          masked_lms_pos.append(index)\n",
        "\n",
        "        init_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(output_tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            init_ids.append(0)\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "\n",
        "        assert len(init_ids) == max_seq_length\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "\n",
        "        if ex_index < 2:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                [str(x) for x in tokens]))\n",
        "            logger.info(\"init_ids: %s\" % \" \".join([str(x) for x in init_ids]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\"masked_lm_labels: %s\" % \" \".join([str(x) for x in masked_lm_labels]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(init_ids=init_ids,\n",
        "                          input_ids=input_ids,\n",
        "                          input_mask=input_mask,\n",
        "                          masked_lm_labels=masked_lm_labels))\n",
        "    return features\n",
        "\n",
        "def prepare_data(features):\n",
        "    all_init_ids = torch.tensor([f.init_ids for f in features], dtype=torch.long)\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_masked_lm_labels = torch.tensor([f.masked_lm_labels for f in features],\n",
        "                                        dtype=torch.long)\n",
        "    tensor_data = TensorDataset(all_init_ids, all_input_ids, all_input_mask, all_masked_lm_labels)\n",
        "    return tensor_data\n",
        "\n",
        "def rev_wordpiece(str):\n",
        "    #print(str)\n",
        "    if len(str) > 1:\n",
        "        for i in range(len(str)-1, 0, -1):\n",
        "            if str[i] == '[PAD]':\n",
        "                str.remove(str[i])\n",
        "            elif len(str[i]) > 1 and str[i][0]=='#' and str[i][1]=='#':\n",
        "                str[i-1] += str[i][2:]\n",
        "                str.remove(str[i])\n",
        "    return \" \".join(str[2:-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma0whXP2vePW"
      },
      "source": [
        "def train_cmodbert_and_augment(args, example_index):\r\n",
        "  task_name = args.task_name\r\n",
        "  os.makedirs(args.output_dir, exist_ok=True)\r\n",
        "\r\n",
        "  random.seed(args.seed)\r\n",
        "  np.random.seed(args.seed)\r\n",
        "  torch.manual_seed(args.seed)\r\n",
        "\r\n",
        "  processor = get_task_processor(task_name, args.data_dir)\r\n",
        "  label_list = processor.get_labels(task_name)\r\n",
        "\r\n",
        "  # load train and dev data\r\n",
        "  train_examples = processor.get_train_examples()\r\n",
        "  dev_examples = processor.get_dev_examples()\r\n",
        "\r\n",
        "  print(train_examples[example_index].guid)\r\n",
        "  print(train_examples[example_index].text_a)\r\n",
        "  print(train_examples[example_index].text_b)\r\n",
        "  print(train_examples[example_index].label)\r\n",
        "\r\n",
        "  tokenizer = BertTokenizer.from_pretrained(BERT_MODEL,\r\n",
        "                                            do_lower_case=True,\r\n",
        "                                            cache_dir=args.cache)\r\n",
        "  model = BertForMaskedLM.from_pretrained(BERT_MODEL,\r\n",
        "                                          cache_dir=args.cache)\r\n",
        "\r\n",
        "  tokenizer.add_tokens(label_list) # 이 부분 좀 의심스러운데\r\n",
        "  model.resize_token_embeddings(len(tokenizer))\r\n",
        "  model.cls = BertOnlyMLMHead(model.config)\r\n",
        "\r\n",
        "  model.to(device)\r\n",
        "\r\n",
        "  # train data\r\n",
        "  train_features = convert_examples_to_features(train_examples, label_list, args.max_seq_length, tokenizer, args.seed)\r\n",
        "  train_data = prepare_data(train_features)\r\n",
        "  train_sampler = RandomSampler(train_data)\r\n",
        "  train_dataloader = DataLoader(train_data,\r\n",
        "                                sampler=train_sampler,\r\n",
        "                                batch_size=args.train_batch_size)\r\n",
        "  \r\n",
        "  # dev data\r\n",
        "  dev_features = convert_examples_to_features(dev_examples,\r\n",
        "                                              label_list,\r\n",
        "                                              args.max_seq_length,\r\n",
        "                                              tokenizer,\r\n",
        "                                              args.seed)\r\n",
        "  dev_data = prepare_data(dev_features)\r\n",
        "  dev_sampler = SequentialSampler(dev_data)\r\n",
        "  dev_dataloader = DataLoader(dev_data,\r\n",
        "                              sampler=dev_sampler,\r\n",
        "                              batch_size=args.train_batch_size)\r\n",
        "  \r\n",
        "  num_train_steps = int(len(train_features) / args.train_batch_size * args.num_train_epochs)\r\n",
        "  logger.info(\"***** Running training *****\")\r\n",
        "  logger.info(\"  Num examples = %d\", len(train_features))\r\n",
        "  logger.info(\"  Batch size = %d\", args.train_batch_size)\r\n",
        "  logger.info(\"  Num steps = %d\", num_train_steps)\r\n",
        "\r\n",
        "  # optimizer\r\n",
        "  t_total = num_train_steps\r\n",
        "  no_decay = ['bias', 'gamma', 'beta', 'LayerNorm.weight']\r\n",
        "  optimizer_grouped_parameters = [\r\n",
        "      {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\r\n",
        "        'weight_decay': 0.01},\r\n",
        "      {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\r\n",
        "        'weight_decay': 0.0}\r\n",
        "  ]\r\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=1e-8)\r\n",
        "\r\n",
        "  best_dev_loss = float('inf')\r\n",
        "  print(best_dev_loss)\r\n",
        "  for epoch in trange(int(args.num_train_epochs), desc=\"Epoch\"):\r\n",
        "    avg_loss = 0.\r\n",
        "    model.train()\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "      batch = tuple(t.to(device) for t in batch)\r\n",
        "      _, input_ids, input_mask, masked_ids = batch\r\n",
        "      inputs = {'input_ids': batch[1],\r\n",
        "                'attention_mask': batch[2],\r\n",
        "                'masked_lm_labels': batch[3]}\r\n",
        "\r\n",
        "      outputs = model(**inputs)\r\n",
        "      loss = outputs[0]\r\n",
        "      # loss = model(input_ids, segment_ids, input_mask, masked_ids)\r\n",
        "      loss.backward()\r\n",
        "      avg_loss += loss.item()\r\n",
        "      optimizer.step()\r\n",
        "      model.zero_grad()\r\n",
        "      if (step + 1) % 50 == 0:\r\n",
        "          print(\"avg_loss: {}\".format(avg_loss / 50))\r\n",
        "      avg_loss = 0.\r\n",
        "\r\n",
        "    # eval on dev after every epoch\r\n",
        "    dev_loss = compute_dev_loss(model, dev_dataloader)\r\n",
        "    print(\"Epoch {}, Dev loss {}\".format(epoch, dev_loss))\r\n",
        "    if dev_loss < best_dev_loss:\r\n",
        "      best_dev_loss = dev_loss\r\n",
        "      print(\"Saving model. Best dev so far {}\".format(best_dev_loss))\r\n",
        "      save_model_path = os.path.join(args.output_dir, 'best_cmodbert.pt')\r\n",
        "      torch.save(model.state_dict(), save_model_path)\r\n",
        "\r\n",
        "  return model, tokenizer, train_data, label_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKT7od6GvlfJ",
        "outputId": "b2cf5961-e3f5-4d46-9f72-b775b8404c7a"
      },
      "source": [
        "model, tokenizer, train_data, label_list = train_cmodbert_and_augment(args, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/11/2021 03:59:13 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "03/11/2021 03:59:13 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "03/11/2021 03:59:13 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/transformers-data-augmentation/datasets/TREC\n",
            "train-1\n",
            "How long is human gestation ?\n",
            "None\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "03/11/2021 03:59:13 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at transformers_cache/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/11/2021 03:59:17 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
            "03/11/2021 03:59:17 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "03/11/2021 03:59:17 - INFO - transformers.tokenization_utils -   Adding 0\n",
            " to the vocabulary\n",
            "03/11/2021 03:59:17 - INFO - transformers.tokenization_utils -   Adding 1\n",
            " to the vocabulary\n",
            "03/11/2021 03:59:17 - INFO - transformers.tokenization_utils -   Adding 2\n",
            " to the vocabulary\n",
            "03/11/2021 03:59:17 - INFO - transformers.tokenization_utils -   Adding 3\n",
            " to the vocabulary\n",
            "03/11/2021 03:59:17 - INFO - transformers.tokenization_utils -   Adding 4\n",
            " to the vocabulary\n",
            "03/11/2021 03:59:17 - INFO - transformers.tokenization_utils -   Adding 5\n",
            " to the vocabulary\n",
            "03/11/2021 03:59:17 - INFO - transformers.tokenization_utils -   Adding label\n",
            " to the vocabulary\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   *** Example ***\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   guid: train-0\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   tokens: [CLS] label sentence [SEP]\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   init_ids: 101 3830 6251 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   input_ids: 101 3830 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   masked_lm_labels: -100 -100 6251 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   *** Example ***\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   guid: train-1\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   tokens: [CLS] 5 how long is human ge ##station ? [SEP]\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   init_ids: 101 1019 2129 2146 2003 2529 16216 20100 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   input_ids: 101 1019 2129 2146 103 2529 16216 20100 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:21 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 2003 -100 -100 -100 1029 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   *** Example ***\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   guid: dev-0\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   tokens: [CLS] label sentence [SEP]\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   init_ids: 101 3830 6251 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   input_ids: 101 3830 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   masked_lm_labels: -100 -100 6251 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   *** Example ***\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   guid: dev-1\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   tokens: [CLS] 0 how do you get rid on wood ##pe ##cker ##s ? [SEP]\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   init_ids: 101 1014 2129 2079 2017 2131 9436 2006 3536 5051 9102 2015 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   input_ids: 101 1014 2129 2079 2017 2131 9436 2006 103 5051 103 2015 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 3536 -100 9102 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/11/2021 03:59:22 - INFO - __main__ -   ***** Running training *****\n",
            "03/11/2021 03:59:22 - INFO - __main__ -     Num examples = 4907\n",
            "03/11/2021 03:59:22 - INFO - __main__ -     Batch size = 8\n",
            "03/11/2021 03:59:22 - INFO - __main__ -     Num steps = 6133\n",
            "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "inf\n",
            "avg_loss: 0.14749713897705077\n",
            "avg_loss: 0.1124178409576416\n",
            "avg_loss: 0.1293332099914551\n",
            "avg_loss: 0.1456339931488037\n",
            "avg_loss: 0.13964256286621093\n",
            "avg_loss: 0.09378203392028808\n",
            "avg_loss: 0.08656529426574706\n",
            "avg_loss: 0.07420523166656494\n",
            "avg_loss: 0.07092886924743652\n",
            "avg_loss: 0.10040390014648437\n",
            "avg_loss: 0.046237034797668455\n",
            "avg_loss: 0.10412364959716797\n",
            "Epoch 0, Dev loss 321.07247614860535\n",
            "Saving model. Best dev so far 321.07247614860535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  10%|█         | 1/10 [01:36<14:24, 96.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.07889072895050049\n",
            "avg_loss: 0.06286872386932373\n",
            "avg_loss: 0.06840457439422608\n",
            "avg_loss: 0.10257291793823242\n",
            "avg_loss: 0.05823854923248291\n",
            "avg_loss: 0.06943430423736573\n",
            "avg_loss: 0.08183790206909179\n",
            "avg_loss: 0.07886219501495362\n",
            "avg_loss: 0.0828990650177002\n",
            "avg_loss: 0.0563795804977417\n",
            "avg_loss: 0.052818312644958496\n",
            "avg_loss: 0.04927875518798828\n",
            "Epoch 1, Dev loss 308.95835864543915\n",
            "Saving model. Best dev so far 308.95835864543915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 2/10 [03:12<12:48, 96.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.05796962738037109\n",
            "avg_loss: 0.05191439151763916\n",
            "avg_loss: 0.05106105327606201\n",
            "avg_loss: 0.06225635528564453\n",
            "avg_loss: 0.052340970039367676\n",
            "avg_loss: 0.04236915588378906\n",
            "avg_loss: 0.028450374603271485\n",
            "avg_loss: 0.06262572288513184\n",
            "avg_loss: 0.05459034919738769\n",
            "avg_loss: 0.04307582855224609\n",
            "avg_loss: 0.04313146114349365\n",
            "avg_loss: 0.06143080234527588\n",
            "Epoch 2, Dev loss 303.44585132598877\n",
            "Saving model. Best dev so far 303.44585132598877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  30%|███       | 3/10 [04:48<11:12, 96.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.028343071937561037\n",
            "avg_loss: 0.03181040048599243\n",
            "avg_loss: 0.03698539733886719\n",
            "avg_loss: 0.03525476455688477\n",
            "avg_loss: 0.05465151786804199\n",
            "avg_loss: 0.03251964092254639\n",
            "avg_loss: 0.038784761428833005\n",
            "avg_loss: 0.055782651901245116\n",
            "avg_loss: 0.044208769798278806\n",
            "avg_loss: 0.031033742427825927\n",
            "avg_loss: 0.015695006847381593\n",
            "avg_loss: 0.038946750164031985\n",
            "Epoch 3, Dev loss 302.87026047706604\n",
            "Saving model. Best dev so far 302.87026047706604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 4/10 [06:23<09:36, 96.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.024553453922271727\n",
            "avg_loss: 0.026608052253723143\n",
            "avg_loss: 0.020166246891021727\n",
            "avg_loss: 0.03499277114868164\n",
            "avg_loss: 0.01984520435333252\n",
            "avg_loss: 0.010063151121139527\n",
            "avg_loss: 0.02372631072998047\n",
            "avg_loss: 0.019359079599380494\n",
            "avg_loss: 0.03165896654129028\n",
            "avg_loss: 0.02938525676727295\n",
            "avg_loss: 0.01866966962814331\n",
            "avg_loss: 0.037000298500061035\n",
            "Epoch 4, Dev loss 301.32491624355316\n",
            "Saving model. Best dev so far 301.32491624355316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 5/10 [08:00<08:00, 96.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.00367341548204422\n",
            "avg_loss: 0.013570754528045655\n",
            "avg_loss: 0.0069470185041427615\n",
            "avg_loss: 0.01803031325340271\n",
            "avg_loss: 0.020314912796020507\n",
            "avg_loss: 0.011440095901489257\n",
            "avg_loss: 0.018651736974716185\n",
            "avg_loss: 0.01015558123588562\n",
            "avg_loss: 0.009552507400512696\n",
            "avg_loss: 0.009389273524284363\n",
            "avg_loss: 0.01786839485168457\n",
            "avg_loss: 0.03133715629577637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 6/10 [09:34<06:22, 95.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5, Dev loss 302.43590664863586\n",
            "avg_loss: 0.00635937511920929\n",
            "avg_loss: 0.011653796434402466\n",
            "avg_loss: 0.011780924797058105\n",
            "avg_loss: 0.006674959659576416\n",
            "avg_loss: 0.009018615484237671\n",
            "avg_loss: 0.004539644420146942\n",
            "avg_loss: 0.01310280203819275\n",
            "avg_loss: 0.004178353250026703\n",
            "avg_loss: 0.012614812850952149\n",
            "avg_loss: 0.01304574966430664\n",
            "avg_loss: 0.004071841835975647\n",
            "avg_loss: 0.010236523151397704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  70%|███████   | 7/10 [11:08<04:45, 95.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6, Dev loss 305.63548243045807\n",
            "avg_loss: 0.004345260858535767\n",
            "avg_loss: 0.003992502093315125\n",
            "avg_loss: 0.0050913399457931515\n",
            "avg_loss: 0.006967033743858337\n",
            "avg_loss: 0.006056959629058838\n",
            "avg_loss: 0.0037680011987686157\n",
            "avg_loss: 0.009455618262290955\n",
            "avg_loss: 0.007782201766967773\n",
            "avg_loss: 0.002526736855506897\n",
            "avg_loss: 0.007161176204681397\n",
            "avg_loss: 0.005452333092689514\n",
            "avg_loss: 0.0030105677247047424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 8/10 [12:43<03:09, 94.95s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7, Dev loss 308.733332157135\n",
            "avg_loss: 0.0045415091514587405\n",
            "avg_loss: 0.0034392452239990234\n",
            "avg_loss: 0.0035231485962867737\n",
            "avg_loss: 0.0014805065095424652\n",
            "avg_loss: 0.004940434098243713\n",
            "avg_loss: 0.002244567275047302\n",
            "avg_loss: 0.0023155677318572997\n",
            "avg_loss: 0.0024924999475479125\n",
            "avg_loss: 0.003725219368934631\n",
            "avg_loss: 0.002823907732963562\n",
            "avg_loss: 0.005153459310531616\n",
            "avg_loss: 0.001724792569875717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  90%|█████████ | 9/10 [14:17<01:34, 94.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8, Dev loss 311.9006907939911\n",
            "avg_loss: 0.0017856520414352417\n",
            "avg_loss: 0.0020895926654338837\n",
            "avg_loss: 0.0010587909817695617\n",
            "avg_loss: 0.001425417810678482\n",
            "avg_loss: 0.0017901211977005005\n",
            "avg_loss: 0.0009386606514453888\n",
            "avg_loss: 0.0010705384612083434\n",
            "avg_loss: 0.001415014863014221\n",
            "avg_loss: 0.0010418861359357834\n",
            "avg_loss: 0.002231907546520233\n",
            "avg_loss: 0.0019370584189891814\n",
            "avg_loss: 0.0009833110123872756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 10/10 [15:51<00:00, 95.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9, Dev loss 312.17053961753845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvxTXUrXJv9n",
        "outputId": "cb15d7e8-f139-431b-d56c-4af18d6d2735"
      },
      "source": [
        "train_sampler = SequentialSampler(train_data)\r\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\r\n",
        "\r\n",
        "best_model_path = os.path.join(args.output_dir, \"best_cmodbert.pt\")\r\n",
        "if os.path.exists(best_model_path):\r\n",
        "  model.load_state_dict(torch.load(best_model_path))\r\n",
        "else:\r\n",
        "  raise ValueError(\"Unable to find the saved model at {}\".format(best_model_path))\r\n",
        "\r\n",
        "save_train_path = os.path.join(args.output_dir, \"cmodbert_aug.tsv\")\r\n",
        "save_train_file = open(save_train_path, 'w')\r\n",
        "\r\n",
        "MASK_id = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\r\n",
        "tsv_writer = csv.writer(save_train_file, delimiter='\\t')\r\n",
        "\r\n",
        "for step, batch in enumerate(train_dataloader):\r\n",
        "  batch = tuple(t.to(device) for t in batch)\r\n",
        "  init_ids, _, input_mask, _ = batch\r\n",
        "  input_lens = [sum(mask).item() for mask in input_mask]\r\n",
        "  masked_idx = np.squeeze(\r\n",
        "    [np.random.randint(2, l, max((l-2) // args.sample_ratio, 1)) for l in input_lens]\r\n",
        "  )\r\n",
        "  \r\n",
        "  for ids, idx in zip(init_ids, masked_idx):\r\n",
        "    ids[idx] = MASK_id\r\n",
        "\r\n",
        "  inputs = {'input_ids': init_ids,\r\n",
        "            'attention_mask': input_mask}\r\n",
        "\r\n",
        "  outputs = model(**inputs)\r\n",
        "  predictions = outputs[0]\r\n",
        "  predictions = F.softmax(predictions / args.temp, dim=2)\r\n",
        "\r\n",
        "  for ids, idx, preds in zip(init_ids, masked_idx, predictions):\r\n",
        "\r\n",
        "    preds = torch.multinomial(preds, args.sample_num, replacement=True)[idx]\r\n",
        "    if len(preds.size()) == 2:\r\n",
        "        preds = torch.transpose(preds, 0, 1)\r\n",
        "    for pred in preds:\r\n",
        "        ids[idx] = pred\r\n",
        "        new_str = tokenizer.convert_ids_to_tokens(ids.cpu().numpy())\r\n",
        "        label = new_str[1]\r\n",
        "        new_str = rev_wordpiece(new_str)\r\n",
        "        tsv_writer.writerow([label, new_str])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFON4PQOM1D2",
        "outputId": "42f0898b-a5c3-45bb-a4a6-6b5ee85f449e"
      },
      "source": [
        "processor2 = get_task_processor(args.task_name, args.data_dir)\r\n",
        "train_examples2 = processor2.get_train_examples()\r\n",
        "train_features2 = convert_examples_to_features(train_examples2[1:2], label_list[1:2], args.max_seq_length, tokenizer, args.seed)\r\n",
        "\r\n",
        "# train_features = convert_examples_to_features(train_examples, label_list, args.max_seq_length, tokenizer, args.seed)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/11/2021 04:15:42 - INFO - __main__ -   *** Example ***\n",
            "03/11/2021 04:15:42 - INFO - __main__ -   guid: train-1\n",
            "03/11/2021 04:15:42 - INFO - __main__ -   tokens: [CLS] 5 how long is human ge ##station ? [SEP]\n",
            "03/11/2021 04:15:42 - INFO - __main__ -   init_ids: 101 1019 2129 2146 2003 2529 16216 20100 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 04:15:42 - INFO - __main__ -   input_ids: 101 1019 2129 2146 103 103 16216 20100 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 04:15:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/11/2021 04:15:42 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 2003 2529 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}