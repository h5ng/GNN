{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cmod_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MDa7JgotH9VN9wGRr2i-FdXLyjp6h_Qd",
      "authorship_tag": "ABX9TyO+Cp2C+MUhQeYZmfV5N/G8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h5ng/GNN/blob/master/cmod_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQeR-cToddu0",
        "outputId": "e8684130-3271-4ad5-9b6b-58c4e8a3dc77"
      },
      "source": [
        "!pip install fairseq==0.9 transformers==2.9"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq==0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 10.4MB/s \n",
            "\u001b[?25hCollecting transformers==2.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 22.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (1.14.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (0.29.22)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (2019.12.20)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (1.8.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.9) (4.41.1)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/59/bb06dd5ca53547d523422d32735585493e0103c992a52a97ba3aa3be33bf/tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 21.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.9) (2.20)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq==0.9) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9) (1.0.1)\n",
            "Building wheels for collected packages: fairseq, sacremoses\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp37-cp37m-linux_x86_64.whl size=2122314 sha256=9684def1d02bb495388b6994089644c11902b78ea69a8c7313e7cfad507eb0fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=158d73d7b15fa2b5116b104ef0ea600636120c4eb8b55bf704eeec09ce075231\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built fairseq sacremoses\n",
            "Installing collected packages: portalocker, sacrebleu, fairseq, tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed fairseq-0.9.0 portalocker-2.0.0 sacrebleu-1.5.1 sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.7.0 transformers-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74Zqw-HtfAbO",
        "outputId": "4360c104-4b47-4f7a-dba2-c47cf7fba5d8"
      },
      "source": [
        "import csv\r\n",
        "import os\r\n",
        "import logging\r\n",
        "import argparse\r\n",
        "import random\r\n",
        "from tqdm import tqdm, trange\r\n",
        "import json\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "from transformers.tokenization_bert import BertTokenizer\r\n",
        "from transformers.modeling_bert import BertForMaskedLM, BertOnlyMLMHead\r\n",
        "\r\n",
        "from transformers import AdamW\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.append('/content/drive/MyDrive/transformers-data-augmentation/bert_aug')\r\n",
        "from data_processors import get_task_processor"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJl0Av8TiXW7"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "BERT_MODEL = 'bert-base-uncased'\r\n",
        "\r\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\r\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\r\n",
        "                    level=logging.INFO)\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0hY52OckZMx"
      },
      "source": [
        "from argparse import Namespace\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "args = {\r\n",
        "    'data_dir': '/content/drive/MyDrive/transformers-data-augmentation/datasets/TREC',\r\n",
        "    'output_dir': 'aug_data',\r\n",
        "    'task_name': 'trec',\r\n",
        "    'max_seq_length': 64,\r\n",
        "    'cache': 'transformers_cache',\r\n",
        "    'train_batch_size': 8,\r\n",
        "    'learning_rate': 4e-5,\r\n",
        "    'num_train_epochs': 10.0,\r\n",
        "    'warmup_proportion': 0.1,\r\n",
        "    'seed': 42,\r\n",
        "    'sample_num': 1,\r\n",
        "    'sample_ratio': 7,\r\n",
        "    'gpu': 0,\r\n",
        "    'temp': 1.0\r\n",
        "}\r\n",
        "args = Namespace(**args)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL_--Cgv2GdU"
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, init_ids, input_ids, input_mask, masked_lm_labels):\n",
        "        self.init_ids = init_ids\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.masked_lm_labels = masked_lm_labels\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, seed=12345):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    # ----\n",
        "    # dupe_factor = 5\n",
        "    masked_lm_prob = 0.15\n",
        "    max_predictions_per_seq = 20\n",
        "    rng = random.Random(seed)\n",
        "\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        modified_example = example.label + \" \" + example.text_a\n",
        "        tokens_a = tokenizer.tokenize(modified_example)\n",
        "        # Account for [CLS] and [SEP] and label with \"- 3\"\n",
        "        if len(tokens_a) > max_seq_length - 3:\n",
        "            tokens_a = tokens_a[0:(max_seq_length - 3)]\n",
        "\n",
        "        # take care of prepending the class label in this code\n",
        "        tokens = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        masked_lm_labels = [-100] * max_seq_length\n",
        "\n",
        "        cand_indexes = []\n",
        "        for (i, token) in enumerate(tokens):\n",
        "            # making sure that masking of # prepended label is avoided\n",
        "            if token == \"[CLS]\" or token == \"[SEP]\" or (token in label_list and i == 1):\n",
        "                continue\n",
        "            cand_indexes.append(i)\n",
        "\n",
        "        rng.shuffle(cand_indexes)\n",
        "        len_cand = len(cand_indexes)\n",
        "\n",
        "        output_tokens = list(tokens)\n",
        "\n",
        "        num_to_predict = min(max_predictions_per_seq,\n",
        "                             max(1, int(round(len(tokens) * masked_lm_prob))))\n",
        "\n",
        "        masked_lms_pos = []\n",
        "        covered_indexes = set()\n",
        "        for index in cand_indexes:\n",
        "            if len(masked_lms_pos) >= num_to_predict:\n",
        "                break\n",
        "            if index in covered_indexes:\n",
        "                continue\n",
        "            covered_indexes.add(index)\n",
        "\n",
        "            masked_token = None\n",
        "            # 80% of the time, replace with [MASK]\n",
        "            if rng.random() < 0.8:\n",
        "                masked_token = \"[MASK]\"\n",
        "            else:\n",
        "                # 10% of the time, keep original\n",
        "                if rng.random() < 0.5:\n",
        "                    masked_token = tokens[index]\n",
        "                # 10% of the time, replace with random word\n",
        "                else:\n",
        "                    masked_token = tokens[cand_indexes[rng.randint(0, len_cand - 1)]]\n",
        "\n",
        "            masked_lm_labels[index] = tokenizer.convert_tokens_to_ids([tokens[index]])[0]\n",
        "            output_tokens[index] = masked_token\n",
        "            masked_lms_pos.append(index)\n",
        "\n",
        "        init_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(output_tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            init_ids.append(0)\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "\n",
        "        assert len(init_ids) == max_seq_length\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "\n",
        "        if ex_index < 2:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                [str(x) for x in tokens]))\n",
        "            logger.info(\"init_ids: %s\" % \" \".join([str(x) for x in init_ids]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\"masked_lm_labels: %s\" % \" \".join([str(x) for x in masked_lm_labels]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(init_ids=init_ids,\n",
        "                          input_ids=input_ids,\n",
        "                          input_mask=input_mask,\n",
        "                          masked_lm_labels=masked_lm_labels))\n",
        "    return features\n",
        "\n",
        "def prepare_data(features):\n",
        "    all_init_ids = torch.tensor([f.init_ids for f in features], dtype=torch.long)\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_masked_lm_labels = torch.tensor([f.masked_lm_labels for f in features],\n",
        "                                        dtype=torch.long)\n",
        "    tensor_data = TensorDataset(all_init_ids, all_input_ids, all_input_mask, all_masked_lm_labels)\n",
        "    return tensor_data\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma0whXP2vePW"
      },
      "source": [
        "def train_cmodbert_and_augment(args, example_index):\r\n",
        "  task_name = args.task_name\r\n",
        "  os.makedirs(args.output_dir, exist_ok=True)\r\n",
        "\r\n",
        "  random.seed(args.seed)\r\n",
        "  np.random.seed(args.seed)\r\n",
        "  torch.manual_seed(args.seed)\r\n",
        "\r\n",
        "  processor = get_task_processor(task_name, args.data_dir)\r\n",
        "  label_list = processor.get_labels(task_name)\r\n",
        "\r\n",
        "  # load train and dev data\r\n",
        "  train_examples = processor.get_train_examples()\r\n",
        "  dev_examples = processor.get_dev_examples()\r\n",
        "\r\n",
        "  print(train_examples[example_index].guid)\r\n",
        "  print(train_examples[example_index].text_a)\r\n",
        "  print(train_examples[example_index].text_b)\r\n",
        "  print(train_examples[example_index].label)\r\n",
        "\r\n",
        "  tokenizer = BertTokenizer.from_pretrained(BERT_MODEL,\r\n",
        "                                            do_lower_case=True,\r\n",
        "                                            cache_dir=args.cache)\r\n",
        "  model = BertForMaskedLM.from_pretrained(BERT_MODEL,\r\n",
        "                                          cache_dir=args.cache)\r\n",
        "\r\n",
        "  tokenizer.add_tokens(label_list) # 이 부분 좀 의심스러운데\r\n",
        "  model.resize_token_embeddings(len(tokenizer))\r\n",
        "  model.cls = BertOnlyMLMHead(model.config)\r\n",
        "\r\n",
        "  model.to(device)\r\n",
        "\r\n",
        "  # train data\r\n",
        "  train_features = convert_examples_to_features(train_examples, label_list, args.max_seq_length, tokenizer, args.seed)\r\n",
        "  train_data = prepare_data(train_features)\r\n",
        "  train_sampler = RandomSampler(train_data)\r\n",
        "  train_dataloader = DataLoader(train_data,\r\n",
        "                                sampler=train_sampler,\r\n",
        "                                batch_size=args.train_batch_size)\r\n",
        "  \r\n",
        "  # dev data\r\n",
        "  dev_features = convert_examples_to_features(dev_examples,\r\n",
        "                                              label_list,\r\n",
        "                                              args.max_seq_length,\r\n",
        "                                              tokenizer,\r\n",
        "                                              args.seed)\r\n",
        "  dev_data = prepare_data(dev_features)\r\n",
        "  dev_sampler = SequentialSampler(dev_data)\r\n",
        "  dev_dataloader = DataLoader(dev_data,\r\n",
        "                              sampler=dev_sampler,\r\n",
        "                              batch_size=args.train_batch_size)\r\n",
        "  \r\n",
        "  num_train_steps = int(len(train_features) / args.train_batch_size * args.num_train_epochs)\r\n",
        "  logger.info(\"***** Running training *****\")\r\n",
        "  logger.info(\"  Num examples = %d\", len(train_features))\r\n",
        "  logger.info(\"  Batch size = %d\", args.train_batch_size)\r\n",
        "  logger.info(\"  Num steps = %d\", num_train_steps)\r\n",
        "\r\n",
        "  # optimizer\r\n",
        "  t_total = num_train_steps\r\n",
        "  no_decay = ['bias', 'gamma', 'beta', 'LayerNorm.weight']\r\n",
        "  optimizer_grouped_parameters = [\r\n",
        "      {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\r\n",
        "        'weight_decay': 0.01},\r\n",
        "      {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\r\n",
        "        'weight_decay': 0.0}\r\n",
        "  ]\r\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=1e-8)\r\n",
        "\r\n",
        "  best_dev_loss = float('inf')\r\n",
        "  print(best_dev_loss)\r\n",
        "  for epoch in trange(int(args.num_train_epochs), desc=\"Epoch\"):\r\n",
        "    avg_loss = 0.\r\n",
        "    model.train()\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "      batch = tuple(t.to(device) for t in batch)\r\n",
        "      _, input_ids, input_mask, masked_ids = batch\r\n",
        "      inputs = {'input_ids': batch[1],\r\n",
        "                'attention_mask': batch[2],\r\n",
        "                'masked_lm_labels': batch[3]}\r\n",
        "\r\n",
        "      outputs = model(**inputs)\r\n",
        "      loss = outputs[0]\r\n",
        "      # loss = model(input_ids, segment_ids, input_mask, masked_ids)\r\n",
        "      loss.backward()\r\n",
        "      avg_loss += loss.item()\r\n",
        "      optimizer.step()\r\n",
        "      model.zero_grad()\r\n",
        "      if (step + 1) % 50 == 0:\r\n",
        "          print(\"avg_loss: {}\".format(avg_loss / 50))\r\n",
        "      avg_loss = 0.\r\n",
        "\r\n",
        "    # eval on dev after every epoch\r\n",
        "    dev_loss = compute_dev_loss(model, dev_dataloader)\r\n",
        "    print(\"Epoch {}, Dev loss {}\".format(epoch, dev_loss))\r\n",
        "    if dev_loss < best_dev_loss:\r\n",
        "      best_dev_loss = dev_loss\r\n",
        "      print(\"Saving model. Best dev so far {}\".format(best_dev_loss))\r\n",
        "      save_model_path = os.path.join(args.output_dir, 'best_cmodbert.pt')\r\n",
        "      torch.save(model.state_dict(), save_model_path)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nKT7od6GvlfJ",
        "outputId": "38e70867-d917-4fd9-ce00-49b5d59d8b81"
      },
      "source": [
        "train_cmodbert_and_augment(args, 1)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/transformers-data-augmentation/datasets/TREC\n",
            "train-1\n",
            "How long is human gestation ?\n",
            "None\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "03/10/2021 16:21:30 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "03/10/2021 16:21:31 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "03/10/2021 16:21:31 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/10/2021 16:21:31 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at transformers_cache/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/10/2021 16:21:35 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
            "03/10/2021 16:21:35 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "03/10/2021 16:21:35 - INFO - transformers.tokenization_utils -   Adding 0\n",
            " to the vocabulary\n",
            "03/10/2021 16:21:35 - INFO - transformers.tokenization_utils -   Adding 1\n",
            " to the vocabulary\n",
            "03/10/2021 16:21:35 - INFO - transformers.tokenization_utils -   Adding 2\n",
            " to the vocabulary\n",
            "03/10/2021 16:21:35 - INFO - transformers.tokenization_utils -   Adding 3\n",
            " to the vocabulary\n",
            "03/10/2021 16:21:35 - INFO - transformers.tokenization_utils -   Adding 4\n",
            " to the vocabulary\n",
            "03/10/2021 16:21:35 - INFO - transformers.tokenization_utils -   Adding 5\n",
            " to the vocabulary\n",
            "03/10/2021 16:21:35 - INFO - transformers.tokenization_utils -   Adding label\n",
            " to the vocabulary\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   *** Example ***\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   guid: train-0\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   tokens: [CLS] label sentence [SEP]\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   init_ids: 101 3830 6251 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   input_ids: 101 3830 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   masked_lm_labels: -100 -100 6251 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   *** Example ***\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   guid: train-1\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   tokens: [CLS] 5 how long is human ge ##station ? [SEP]\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   init_ids: 101 1019 2129 2146 2003 2529 16216 20100 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   input_ids: 101 1019 2129 2146 103 2529 16216 20100 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:35 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 2003 -100 -100 -100 1029 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   *** Example ***\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   guid: dev-0\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   tokens: [CLS] label sentence [SEP]\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   init_ids: 101 3830 6251 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   input_ids: 101 3830 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   masked_lm_labels: -100 -100 6251 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   *** Example ***\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   guid: dev-1\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   tokens: [CLS] 0 how do you get rid on wood ##pe ##cker ##s ? [SEP]\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   init_ids: 101 1014 2129 2079 2017 2131 9436 2006 3536 5051 9102 2015 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   input_ids: 101 1014 2129 2079 2017 2131 9436 2006 103 5051 103 2015 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/10/2021 16:21:37 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 3536 -100 9102 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/10/2021 16:21:38 - INFO - __main__ -   ***** Running training *****\n",
            "03/10/2021 16:21:38 - INFO - __main__ -     Num examples = 4907\n",
            "03/10/2021 16:21:38 - INFO - __main__ -     Batch size = 8\n",
            "03/10/2021 16:21:38 - INFO - __main__ -     Num steps = 6133\n",
            "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-a8c8d236d117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_cmodbert_and_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-53f8c5021a61>\u001b[0m in \u001b[0;36mtrain_cmodbert_and_augment\u001b[0;34m(args, example_index)\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0;31m# loss = model(input_ids, segment_ids, input_mask, masked_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m       \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}