{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAT_with_pytorch2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXxMcnlRQvjsFfa/cuH0T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h5ng/GNN/blob/master/GAT_with_pytorch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqU93uSFvZ-U"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efED_xmi-W5v"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0XkuseSvbAV"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "def encode_onehot(labels):\n",
        "  # The classes must be sorted before encoding to enable static class encoding.\n",
        "  # In other words, make sure the first class always maps to index 0.\n",
        "  classes = sorted(list(set(labels)))\n",
        "  classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "  labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "  return labels_onehot\n",
        "\n",
        "def normalize_features(mx):\n",
        "  \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "  rowsum = np.array(mx.sum(1))\n",
        "  r_inv = np.power(rowsum, -1).flatten()\n",
        "  r_inv[np.isinf(r_inv)] = 0.\n",
        "  r_mat_inv = sp.diags(r_inv)\n",
        "  mx = r_mat_inv.dot(mx)\n",
        "  return mx\n",
        "\n",
        "def normalize_adj(mx):\n",
        "  \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "  rowsum = np.array(mx.sum(1))\n",
        "  r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "  r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "  r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "  return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "def accuracy(output, labels):\n",
        "  preds = output.max(1)[1].type_as(labels)\n",
        "  correct = preds.eq(labels).double()\n",
        "  correct = correct.sum()\n",
        "  return correct / len(labels)\n",
        "\n",
        "def load_data(path=\"./sample_data/cora/\", dataset=\"cora\"):\n",
        "  print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "  # <paper_id> <word_attributes>+ <class_label>\n",
        "  idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
        "  features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "  labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "  # build graph\n",
        "  idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "  idx_map = {j: i for i, j in enumerate(idx)}\n",
        "  edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
        "  edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
        "  adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
        "\n",
        "  # build symmetric adjacency matrix\n",
        "  adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "  features = normalize_features(features)\n",
        "  adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "  idx_train = range(140)\n",
        "  idx_val = range(200, 500)\n",
        "  idx_test = range(500, 1500)\n",
        "\n",
        "  adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "  features = torch.FloatTensor(np.array(features.todense()))\n",
        "  labels = torch.LongTensor(np.where(labels)[1])\n",
        "\n",
        "  idx_train = torch.LongTensor(idx_train)\n",
        "  idx_val = torch.LongTensor(idx_val)\n",
        "  idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "  return adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y28qjRV5vbIm",
        "outputId": "7bf15d17-fdbd-4cd6-f189-3dd7cb0d72f8"
      },
      "source": [
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoPI_SCz_xtS",
        "outputId": "c0b0c323-9446-4fdc-d2e1-ca2ea5ba1a5f"
      },
      "source": [
        "print('adj', adj.shape)\n",
        "print('features', features.shape)\n",
        "print('labels', labels.shape)\n",
        "print('idx_train', idx_train.shape)\n",
        "print('idx_val', idx_val.shape)\n",
        "print('idx_test', idx_test.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adj torch.Size([2708, 2708])\n",
            "features torch.Size([2708, 1433])\n",
            "labels torch.Size([2708])\n",
            "idx_train torch.Size([140])\n",
            "idx_val torch.Size([300])\n",
            "idx_test torch.Size([1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DPAV6iSOJiu"
      },
      "source": [
        "# Training settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mmhYxBzAiAy"
      },
      "source": [
        "import random\n",
        "\n",
        "seed = 72\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "dropout = 0.6\n",
        "alpha = 0.2\n",
        "hidden = 8\n",
        "nb_heads = 8\n",
        "lr = 0.05\n",
        "weight_decay=5e-4\n",
        "fastmode = False\n",
        "epochs = 10000\n",
        "patience = 100"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMRbcghhQ4uz"
      },
      "source": [
        "# GAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbuxc_MiO_BD"
      },
      "source": [
        "# GAT model\n",
        "class GAT(nn.Module):\n",
        "  def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "    \"\"\"Dense version of GAT.\"\"\"\n",
        "    super(GAT, self).__init__()\n",
        "    self.dropout = dropout\n",
        "    \n",
        "    self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
        "    for i, attention in enumerate(self.attentions):\n",
        "      self.add_module('attention_{}'.format(i), attention)\n",
        "    \n",
        "    self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
        "\n",
        "\n",
        "  def forward(self, x, adj):\n",
        "    x = F.dropout(x, self.dropout, training=self.training)\n",
        "    x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "    x = F.dropout(x, self.dropout, training=self.training)\n",
        "    x = F.elu(self.out_att(x, adj))\n",
        "    return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cuzLqR4Q6WZ"
      },
      "source": [
        "# Graph Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO4sye1tQfPr"
      },
      "source": [
        "class GraphAttentionLayer(nn.Module):\n",
        "  \"\"\" Simple Gat layer \"\"\"\n",
        "  def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "    super(GraphAttentionLayer, self).__init__()\n",
        "    self.dropout = dropout\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.alpha = alpha\n",
        "    self.concat = concat\n",
        "\n",
        "    self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "    nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "    self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "    nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "    self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "\n",
        "  def forward(self, h, adj):\n",
        "    # W.shape: (in_features, out_features)\n",
        "    # h.shape: (N, in_features)\n",
        "    # Wh.shape: (N, out_features)\n",
        "    Wh = torch.mm(h, self.W) \n",
        "    a_input = self._prepare_attentional_mechanism_input(Wh)\n",
        "    e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "    \n",
        "    zero_vec = -9e15*torch.ones_like(e)\n",
        "    attention = torch.where(adj > 0, e, zero_vec)\n",
        "    attention = F.softmax(attention, dim=1)\n",
        "    attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "    h_prime = torch.matmul(attention, Wh)\n",
        "\n",
        "    if self.concat:\n",
        "        return F.elu(h_prime)\n",
        "    else:\n",
        "        return h_prime\n",
        "\n",
        "  def _prepare_attentional_mechanism_input(self, Wh):\n",
        "    N = Wh.size()[0] # number of nodes\n",
        "    Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\n",
        "    Wh_repeated_alternating = Wh.repeat(N, 1)\n",
        "\n",
        "    all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
        "    return all_combinations_matrix.view(N, N, 2 * self.out_features)\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX6liWKfO_qr"
      },
      "source": [
        "# Model and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eeENEmtdIiL"
      },
      "source": [
        "model = GAT(\n",
        "    nfeat=features.shape[1],\n",
        "    nhid=hidden,\n",
        "    nclass=int(labels.max()) + 1,\n",
        "    dropout=dropout,\n",
        "    alpha=alpha,\n",
        "    nheads=nb_heads)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=lr,\n",
        "                       weight_decay=weight_decay)\n",
        "\n",
        "model.cuda()\n",
        "features = features.cuda()\n",
        "adj = adj.cuda()\n",
        "labels = labels.cuda()\n",
        "idx_train = idx_train.cuda()\n",
        "idx_val = idx_val.cuda()\n",
        "idx_test = idx_test.cuda()\n",
        "\n",
        "# 미분할 변수 선택?\n",
        "features, adj, labels = Variable(features), Variable(adj), Variable(labels)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2VvF82lk75e"
      },
      "source": [
        "# train 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzn6UiAek6rV"
      },
      "source": [
        "def train(epoch):\n",
        "  t = time.time()\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  output = model(features, adj)\n",
        "  loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "  acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "  loss_train.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if not fastmode:\n",
        "    # Evaluate validation set performance separately,\n",
        "    # deactivates dropout during validation run.\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "\n",
        "  loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "  acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "  print('Epoch: {:04d}'.format(epoch+1),\n",
        "        'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "        'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
        "        'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "        'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
        "        'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "  return loss_val.data.item()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTo-WN9S7UlF"
      },
      "source": [
        "def compute_test():\n",
        "  model.eval()\n",
        "  output = model(features, adj)\n",
        "  loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "  acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "  print(\"Test set results:\",\n",
        "        \"loss= {:.4f}\".format(loss_test.data[0]),\n",
        "        \"accuracy= {:.4f}\".format(acc_test.data[0]))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3mYTNNuxJNu",
        "outputId": "12a111b7-f0f5-4283-822f-4972a1c11471"
      },
      "source": [
        "import time\n",
        "import glob\n",
        "import os\n",
        "\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = epochs + 1\n",
        "best_epoch = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss_values.append(train(epoch))\n",
        "\n",
        "  torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "  if loss_values[-1] < best:\n",
        "    best = loss_values[-1]\n",
        "    best_epoch = epoch\n",
        "    bad_counter = 0\n",
        "  else:\n",
        "    bad_counter += 1\n",
        "  \n",
        "  if bad_counter == patience:\n",
        "    break\n",
        "\n",
        "  files = glob.glob('*.pkl')\n",
        "  for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb < best_epoch:\n",
        "      os.remove(file)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 1.9519 acc_train: 0.0786 loss_val: 1.8714 acc_val: 0.5767 time: 0.4394s\n",
            "Epoch: 0002 loss_train: 1.8412 acc_train: 0.5071 loss_val: 1.7757 acc_val: 0.5733 time: 0.4316s\n",
            "Epoch: 0003 loss_train: 1.7346 acc_train: 0.5429 loss_val: 1.6643 acc_val: 0.5767 time: 0.4303s\n",
            "Epoch: 0004 loss_train: 1.6407 acc_train: 0.5429 loss_val: 1.5510 acc_val: 0.5900 time: 0.4265s\n",
            "Epoch: 0005 loss_train: 1.5116 acc_train: 0.5571 loss_val: 1.4412 acc_val: 0.6167 time: 0.4245s\n",
            "Epoch: 0006 loss_train: 1.4349 acc_train: 0.5929 loss_val: 1.3388 acc_val: 0.6800 time: 0.4291s\n",
            "Epoch: 0007 loss_train: 1.3208 acc_train: 0.6429 loss_val: 1.2419 acc_val: 0.7200 time: 0.4267s\n",
            "Epoch: 0008 loss_train: 1.2011 acc_train: 0.6357 loss_val: 1.1547 acc_val: 0.7433 time: 0.4294s\n",
            "Epoch: 0009 loss_train: 1.1871 acc_train: 0.6857 loss_val: 1.0764 acc_val: 0.7667 time: 0.4268s\n",
            "Epoch: 0010 loss_train: 1.1242 acc_train: 0.6786 loss_val: 1.0069 acc_val: 0.7933 time: 0.4266s\n",
            "Epoch: 0011 loss_train: 1.0543 acc_train: 0.6929 loss_val: 0.9489 acc_val: 0.8033 time: 0.4259s\n",
            "Epoch: 0012 loss_train: 0.9399 acc_train: 0.7357 loss_val: 0.9020 acc_val: 0.8033 time: 0.4277s\n",
            "Epoch: 0013 loss_train: 1.0590 acc_train: 0.7071 loss_val: 0.8683 acc_val: 0.8067 time: 0.4252s\n",
            "Epoch: 0014 loss_train: 0.9747 acc_train: 0.7143 loss_val: 0.8423 acc_val: 0.8133 time: 0.4290s\n",
            "Epoch: 0015 loss_train: 0.8560 acc_train: 0.7714 loss_val: 0.8236 acc_val: 0.8167 time: 0.4309s\n",
            "Epoch: 0016 loss_train: 0.8751 acc_train: 0.7714 loss_val: 0.8095 acc_val: 0.8167 time: 0.4261s\n",
            "Epoch: 0017 loss_train: 0.8707 acc_train: 0.7500 loss_val: 0.7959 acc_val: 0.8267 time: 0.4255s\n",
            "Epoch: 0018 loss_train: 0.8978 acc_train: 0.7714 loss_val: 0.7833 acc_val: 0.8233 time: 0.4289s\n",
            "Epoch: 0019 loss_train: 0.8058 acc_train: 0.7357 loss_val: 0.7689 acc_val: 0.8233 time: 0.4254s\n",
            "Epoch: 0020 loss_train: 0.7568 acc_train: 0.7929 loss_val: 0.7590 acc_val: 0.8267 time: 0.4247s\n",
            "Epoch: 0021 loss_train: 0.8178 acc_train: 0.7643 loss_val: 0.7488 acc_val: 0.8267 time: 0.4281s\n",
            "Epoch: 0022 loss_train: 0.8649 acc_train: 0.7857 loss_val: 0.7424 acc_val: 0.8167 time: 0.4313s\n",
            "Epoch: 0023 loss_train: 0.7269 acc_train: 0.8000 loss_val: 0.7361 acc_val: 0.8133 time: 0.4257s\n",
            "Epoch: 0024 loss_train: 0.7688 acc_train: 0.7714 loss_val: 0.7297 acc_val: 0.8233 time: 0.4362s\n",
            "Epoch: 0025 loss_train: 0.7351 acc_train: 0.7714 loss_val: 0.7258 acc_val: 0.8167 time: 0.4263s\n",
            "Epoch: 0026 loss_train: 0.7740 acc_train: 0.7429 loss_val: 0.7237 acc_val: 0.8167 time: 0.4263s\n",
            "Epoch: 0027 loss_train: 0.6932 acc_train: 0.7929 loss_val: 0.7220 acc_val: 0.8167 time: 0.4271s\n",
            "Epoch: 0028 loss_train: 0.7265 acc_train: 0.8071 loss_val: 0.7207 acc_val: 0.8067 time: 0.4255s\n",
            "Epoch: 0029 loss_train: 0.7097 acc_train: 0.7929 loss_val: 0.7188 acc_val: 0.8067 time: 0.4301s\n",
            "Epoch: 0030 loss_train: 0.8135 acc_train: 0.7500 loss_val: 0.7199 acc_val: 0.8067 time: 0.4227s\n",
            "Epoch: 0031 loss_train: 0.6990 acc_train: 0.8286 loss_val: 0.7205 acc_val: 0.8067 time: 0.4293s\n",
            "Epoch: 0032 loss_train: 0.7139 acc_train: 0.7929 loss_val: 0.7205 acc_val: 0.8133 time: 0.4293s\n",
            "Epoch: 0033 loss_train: 0.6214 acc_train: 0.8214 loss_val: 0.7197 acc_val: 0.8167 time: 0.4269s\n",
            "Epoch: 0034 loss_train: 0.7652 acc_train: 0.7857 loss_val: 0.7156 acc_val: 0.8133 time: 0.4266s\n",
            "Epoch: 0035 loss_train: 0.6809 acc_train: 0.7929 loss_val: 0.7111 acc_val: 0.8167 time: 0.4274s\n",
            "Epoch: 0036 loss_train: 0.8177 acc_train: 0.7214 loss_val: 0.7074 acc_val: 0.8100 time: 0.4307s\n",
            "Epoch: 0037 loss_train: 0.7908 acc_train: 0.7643 loss_val: 0.7054 acc_val: 0.8167 time: 0.4297s\n",
            "Epoch: 0038 loss_train: 0.6494 acc_train: 0.8000 loss_val: 0.7070 acc_val: 0.8100 time: 0.4258s\n",
            "Epoch: 0039 loss_train: 0.5640 acc_train: 0.8643 loss_val: 0.7076 acc_val: 0.8167 time: 0.4244s\n",
            "Epoch: 0040 loss_train: 0.6412 acc_train: 0.8143 loss_val: 0.7040 acc_val: 0.8133 time: 0.4253s\n",
            "Epoch: 0041 loss_train: 0.6818 acc_train: 0.7500 loss_val: 0.6996 acc_val: 0.8100 time: 0.4331s\n",
            "Epoch: 0042 loss_train: 0.6719 acc_train: 0.8071 loss_val: 0.6936 acc_val: 0.8100 time: 0.4252s\n",
            "Epoch: 0043 loss_train: 0.7185 acc_train: 0.7714 loss_val: 0.6897 acc_val: 0.8033 time: 0.4297s\n",
            "Epoch: 0044 loss_train: 0.6122 acc_train: 0.8214 loss_val: 0.6859 acc_val: 0.8067 time: 0.4254s\n",
            "Epoch: 0045 loss_train: 0.6769 acc_train: 0.7786 loss_val: 0.6826 acc_val: 0.8100 time: 0.4266s\n",
            "Epoch: 0046 loss_train: 0.6649 acc_train: 0.8000 loss_val: 0.6763 acc_val: 0.8167 time: 0.4305s\n",
            "Epoch: 0047 loss_train: 0.6762 acc_train: 0.8071 loss_val: 0.6692 acc_val: 0.8233 time: 0.4269s\n",
            "Epoch: 0048 loss_train: 0.7139 acc_train: 0.7929 loss_val: 0.6630 acc_val: 0.8267 time: 0.4300s\n",
            "Epoch: 0049 loss_train: 0.6538 acc_train: 0.8071 loss_val: 0.6576 acc_val: 0.8300 time: 0.4284s\n",
            "Epoch: 0050 loss_train: 0.6709 acc_train: 0.7786 loss_val: 0.6523 acc_val: 0.8300 time: 0.4261s\n",
            "Epoch: 0051 loss_train: 0.6489 acc_train: 0.8000 loss_val: 0.6468 acc_val: 0.8300 time: 0.4240s\n",
            "Epoch: 0052 loss_train: 0.7064 acc_train: 0.7857 loss_val: 0.6428 acc_val: 0.8367 time: 0.4302s\n",
            "Epoch: 0053 loss_train: 0.7424 acc_train: 0.7571 loss_val: 0.6395 acc_val: 0.8367 time: 0.4297s\n",
            "Epoch: 0054 loss_train: 0.7322 acc_train: 0.7714 loss_val: 0.6372 acc_val: 0.8300 time: 0.4272s\n",
            "Epoch: 0055 loss_train: 0.6685 acc_train: 0.7857 loss_val: 0.6357 acc_val: 0.8300 time: 0.4316s\n",
            "Epoch: 0056 loss_train: 0.7512 acc_train: 0.7571 loss_val: 0.6364 acc_val: 0.8267 time: 0.4249s\n",
            "Epoch: 0057 loss_train: 0.7011 acc_train: 0.7929 loss_val: 0.6376 acc_val: 0.8300 time: 0.4271s\n",
            "Epoch: 0058 loss_train: 0.6492 acc_train: 0.7714 loss_val: 0.6382 acc_val: 0.8267 time: 0.4252s\n",
            "Epoch: 0059 loss_train: 0.6821 acc_train: 0.7857 loss_val: 0.6381 acc_val: 0.8333 time: 0.4240s\n",
            "Epoch: 0060 loss_train: 0.7011 acc_train: 0.7857 loss_val: 0.6401 acc_val: 0.8400 time: 0.4428s\n",
            "Epoch: 0061 loss_train: 0.5697 acc_train: 0.8286 loss_val: 0.6435 acc_val: 0.8300 time: 0.4258s\n",
            "Epoch: 0062 loss_train: 0.5430 acc_train: 0.8714 loss_val: 0.6468 acc_val: 0.8300 time: 0.4333s\n",
            "Epoch: 0063 loss_train: 0.6387 acc_train: 0.8000 loss_val: 0.6489 acc_val: 0.8300 time: 0.4262s\n",
            "Epoch: 0064 loss_train: 0.7200 acc_train: 0.7500 loss_val: 0.6505 acc_val: 0.8300 time: 0.4275s\n",
            "Epoch: 0065 loss_train: 0.6548 acc_train: 0.7714 loss_val: 0.6519 acc_val: 0.8200 time: 0.4255s\n",
            "Epoch: 0066 loss_train: 0.7370 acc_train: 0.7643 loss_val: 0.6574 acc_val: 0.8033 time: 0.4310s\n",
            "Epoch: 0067 loss_train: 0.6707 acc_train: 0.7714 loss_val: 0.6635 acc_val: 0.8000 time: 0.4317s\n",
            "Epoch: 0068 loss_train: 0.6662 acc_train: 0.7571 loss_val: 0.6690 acc_val: 0.7933 time: 0.4269s\n",
            "Epoch: 0069 loss_train: 0.6350 acc_train: 0.8143 loss_val: 0.6677 acc_val: 0.8000 time: 0.4326s\n",
            "Epoch: 0070 loss_train: 0.6805 acc_train: 0.8071 loss_val: 0.6678 acc_val: 0.8000 time: 0.4296s\n",
            "Epoch: 0071 loss_train: 0.6406 acc_train: 0.7857 loss_val: 0.6710 acc_val: 0.8067 time: 0.4293s\n",
            "Epoch: 0072 loss_train: 0.6400 acc_train: 0.8000 loss_val: 0.6745 acc_val: 0.8200 time: 0.4306s\n",
            "Epoch: 0073 loss_train: 0.6347 acc_train: 0.7929 loss_val: 0.6777 acc_val: 0.8100 time: 0.4261s\n",
            "Epoch: 0074 loss_train: 0.6339 acc_train: 0.8143 loss_val: 0.6810 acc_val: 0.8033 time: 0.4271s\n",
            "Epoch: 0075 loss_train: 0.6580 acc_train: 0.8214 loss_val: 0.6803 acc_val: 0.8133 time: 0.4286s\n",
            "Epoch: 0076 loss_train: 0.6564 acc_train: 0.7786 loss_val: 0.6772 acc_val: 0.8167 time: 0.4348s\n",
            "Epoch: 0077 loss_train: 0.7058 acc_train: 0.7571 loss_val: 0.6740 acc_val: 0.8167 time: 0.4307s\n",
            "Epoch: 0078 loss_train: 0.6001 acc_train: 0.7714 loss_val: 0.6709 acc_val: 0.8167 time: 0.4308s\n",
            "Epoch: 0079 loss_train: 0.7097 acc_train: 0.7714 loss_val: 0.6650 acc_val: 0.8233 time: 0.4277s\n",
            "Epoch: 0080 loss_train: 0.7102 acc_train: 0.7714 loss_val: 0.6584 acc_val: 0.8233 time: 0.4254s\n",
            "Epoch: 0081 loss_train: 0.6802 acc_train: 0.8071 loss_val: 0.6550 acc_val: 0.8200 time: 0.4285s\n",
            "Epoch: 0082 loss_train: 0.6023 acc_train: 0.8143 loss_val: 0.6523 acc_val: 0.8233 time: 0.4259s\n",
            "Epoch: 0083 loss_train: 0.7353 acc_train: 0.7429 loss_val: 0.6493 acc_val: 0.8200 time: 0.4291s\n",
            "Epoch: 0084 loss_train: 0.5732 acc_train: 0.8571 loss_val: 0.6479 acc_val: 0.8200 time: 0.4339s\n",
            "Epoch: 0085 loss_train: 0.6371 acc_train: 0.8143 loss_val: 0.6467 acc_val: 0.8300 time: 0.4293s\n",
            "Epoch: 0086 loss_train: 0.5779 acc_train: 0.8429 loss_val: 0.6470 acc_val: 0.8267 time: 0.4318s\n",
            "Epoch: 0087 loss_train: 0.5103 acc_train: 0.8786 loss_val: 0.6476 acc_val: 0.8200 time: 0.4270s\n",
            "Epoch: 0088 loss_train: 0.7256 acc_train: 0.7500 loss_val: 0.6470 acc_val: 0.8167 time: 0.4276s\n",
            "Epoch: 0089 loss_train: 0.6483 acc_train: 0.7714 loss_val: 0.6481 acc_val: 0.8200 time: 0.4273s\n",
            "Epoch: 0090 loss_train: 0.5700 acc_train: 0.8214 loss_val: 0.6520 acc_val: 0.8233 time: 0.4311s\n",
            "Epoch: 0091 loss_train: 0.5141 acc_train: 0.8500 loss_val: 0.6606 acc_val: 0.8167 time: 0.4281s\n",
            "Epoch: 0092 loss_train: 0.6436 acc_train: 0.8000 loss_val: 0.6707 acc_val: 0.8133 time: 0.4319s\n",
            "Epoch: 0093 loss_train: 0.6535 acc_train: 0.8071 loss_val: 0.6683 acc_val: 0.8200 time: 0.4270s\n",
            "Epoch: 0094 loss_train: 0.6725 acc_train: 0.7500 loss_val: 0.6566 acc_val: 0.8200 time: 0.4256s\n",
            "Epoch: 0095 loss_train: 0.7227 acc_train: 0.8071 loss_val: 0.6463 acc_val: 0.8133 time: 0.4294s\n",
            "Epoch: 0096 loss_train: 0.6720 acc_train: 0.7857 loss_val: 0.6408 acc_val: 0.8133 time: 0.4270s\n",
            "Epoch: 0097 loss_train: 0.6939 acc_train: 0.8214 loss_val: 0.6398 acc_val: 0.8200 time: 0.4322s\n",
            "Epoch: 0098 loss_train: 0.5743 acc_train: 0.8143 loss_val: 0.6398 acc_val: 0.8167 time: 0.4312s\n",
            "Epoch: 0099 loss_train: 0.7308 acc_train: 0.7571 loss_val: 0.6425 acc_val: 0.8133 time: 0.4305s\n",
            "Epoch: 0100 loss_train: 0.6183 acc_train: 0.8000 loss_val: 0.6449 acc_val: 0.8133 time: 0.4296s\n",
            "Epoch: 0101 loss_train: 0.6572 acc_train: 0.7786 loss_val: 0.6464 acc_val: 0.8133 time: 0.4281s\n",
            "Epoch: 0102 loss_train: 0.7804 acc_train: 0.7571 loss_val: 0.6507 acc_val: 0.8133 time: 0.4254s\n",
            "Epoch: 0103 loss_train: 0.6560 acc_train: 0.7929 loss_val: 0.6563 acc_val: 0.8133 time: 0.4251s\n",
            "Epoch: 0104 loss_train: 0.7009 acc_train: 0.7571 loss_val: 0.6624 acc_val: 0.8167 time: 0.4287s\n",
            "Epoch: 0105 loss_train: 0.5844 acc_train: 0.8071 loss_val: 0.6658 acc_val: 0.8233 time: 0.4272s\n",
            "Epoch: 0106 loss_train: 0.7255 acc_train: 0.7429 loss_val: 0.6670 acc_val: 0.8267 time: 0.4255s\n",
            "Epoch: 0107 loss_train: 0.5669 acc_train: 0.8286 loss_val: 0.6677 acc_val: 0.8267 time: 0.4265s\n",
            "Epoch: 0108 loss_train: 0.6963 acc_train: 0.7714 loss_val: 0.6713 acc_val: 0.8200 time: 0.4417s\n",
            "Epoch: 0109 loss_train: 0.7048 acc_train: 0.7857 loss_val: 0.6759 acc_val: 0.8200 time: 0.4318s\n",
            "Epoch: 0110 loss_train: 0.7671 acc_train: 0.7286 loss_val: 0.6771 acc_val: 0.8133 time: 0.4308s\n",
            "Epoch: 0111 loss_train: 0.6733 acc_train: 0.8143 loss_val: 0.6732 acc_val: 0.8133 time: 0.4323s\n",
            "Epoch: 0112 loss_train: 0.6963 acc_train: 0.7929 loss_val: 0.6666 acc_val: 0.8167 time: 0.4313s\n",
            "Epoch: 0113 loss_train: 0.6753 acc_train: 0.7714 loss_val: 0.6612 acc_val: 0.8133 time: 0.4303s\n",
            "Epoch: 0114 loss_train: 0.6379 acc_train: 0.7929 loss_val: 0.6562 acc_val: 0.8233 time: 0.4254s\n",
            "Epoch: 0115 loss_train: 0.6156 acc_train: 0.8429 loss_val: 0.6513 acc_val: 0.8267 time: 0.4241s\n",
            "Epoch: 0116 loss_train: 0.5935 acc_train: 0.8500 loss_val: 0.6478 acc_val: 0.8267 time: 0.4251s\n",
            "Epoch: 0117 loss_train: 0.6860 acc_train: 0.7643 loss_val: 0.6440 acc_val: 0.8267 time: 0.4281s\n",
            "Epoch: 0118 loss_train: 0.6327 acc_train: 0.8071 loss_val: 0.6400 acc_val: 0.8267 time: 0.4344s\n",
            "Epoch: 0119 loss_train: 0.6464 acc_train: 0.7786 loss_val: 0.6353 acc_val: 0.8267 time: 0.4299s\n",
            "Epoch: 0120 loss_train: 0.6360 acc_train: 0.7929 loss_val: 0.6319 acc_val: 0.8300 time: 0.4293s\n",
            "Epoch: 0121 loss_train: 0.5882 acc_train: 0.8214 loss_val: 0.6306 acc_val: 0.8233 time: 0.4274s\n",
            "Epoch: 0122 loss_train: 0.7193 acc_train: 0.7571 loss_val: 0.6337 acc_val: 0.8233 time: 0.4302s\n",
            "Epoch: 0123 loss_train: 0.6243 acc_train: 0.8071 loss_val: 0.6379 acc_val: 0.8200 time: 0.4283s\n",
            "Epoch: 0124 loss_train: 0.5244 acc_train: 0.8429 loss_val: 0.6398 acc_val: 0.8100 time: 0.4271s\n",
            "Epoch: 0125 loss_train: 0.6456 acc_train: 0.8286 loss_val: 0.6382 acc_val: 0.8100 time: 0.4297s\n",
            "Epoch: 0126 loss_train: 0.7178 acc_train: 0.7357 loss_val: 0.6363 acc_val: 0.8033 time: 0.4257s\n",
            "Epoch: 0127 loss_train: 0.6992 acc_train: 0.7571 loss_val: 0.6319 acc_val: 0.8067 time: 0.4317s\n",
            "Epoch: 0128 loss_train: 0.7626 acc_train: 0.7500 loss_val: 0.6274 acc_val: 0.8133 time: 0.4303s\n",
            "Epoch: 0129 loss_train: 0.7733 acc_train: 0.7286 loss_val: 0.6257 acc_val: 0.8167 time: 0.4254s\n",
            "Epoch: 0130 loss_train: 0.6324 acc_train: 0.7929 loss_val: 0.6257 acc_val: 0.8200 time: 0.4273s\n",
            "Epoch: 0131 loss_train: 0.6906 acc_train: 0.7929 loss_val: 0.6268 acc_val: 0.8300 time: 0.4263s\n",
            "Epoch: 0132 loss_train: 0.6271 acc_train: 0.8143 loss_val: 0.6285 acc_val: 0.8300 time: 0.4326s\n",
            "Epoch: 0133 loss_train: 0.6406 acc_train: 0.7643 loss_val: 0.6307 acc_val: 0.8300 time: 0.4274s\n",
            "Epoch: 0134 loss_train: 0.6839 acc_train: 0.8071 loss_val: 0.6352 acc_val: 0.8267 time: 0.4322s\n",
            "Epoch: 0135 loss_train: 0.6090 acc_train: 0.8071 loss_val: 0.6412 acc_val: 0.8333 time: 0.4293s\n",
            "Epoch: 0136 loss_train: 0.6932 acc_train: 0.7786 loss_val: 0.6463 acc_val: 0.8367 time: 0.4289s\n",
            "Epoch: 0137 loss_train: 0.7228 acc_train: 0.7714 loss_val: 0.6520 acc_val: 0.8233 time: 0.4292s\n",
            "Epoch: 0138 loss_train: 0.6205 acc_train: 0.7786 loss_val: 0.6597 acc_val: 0.8200 time: 0.4346s\n",
            "Epoch: 0139 loss_train: 0.5909 acc_train: 0.7929 loss_val: 0.6659 acc_val: 0.8167 time: 0.4301s\n",
            "Epoch: 0140 loss_train: 0.6652 acc_train: 0.8429 loss_val: 0.6707 acc_val: 0.8133 time: 0.4264s\n",
            "Epoch: 0141 loss_train: 0.7085 acc_train: 0.7571 loss_val: 0.6682 acc_val: 0.8167 time: 0.4297s\n",
            "Epoch: 0142 loss_train: 0.5799 acc_train: 0.8643 loss_val: 0.6652 acc_val: 0.8167 time: 0.4290s\n",
            "Epoch: 0143 loss_train: 0.6729 acc_train: 0.7714 loss_val: 0.6603 acc_val: 0.8233 time: 0.4328s\n",
            "Epoch: 0144 loss_train: 0.6530 acc_train: 0.8143 loss_val: 0.6567 acc_val: 0.8267 time: 0.4301s\n",
            "Epoch: 0145 loss_train: 0.6146 acc_train: 0.8286 loss_val: 0.6568 acc_val: 0.8333 time: 0.4288s\n",
            "Epoch: 0146 loss_train: 0.6528 acc_train: 0.7929 loss_val: 0.6570 acc_val: 0.8300 time: 0.4325s\n",
            "Epoch: 0147 loss_train: 0.6600 acc_train: 0.7857 loss_val: 0.6572 acc_val: 0.8267 time: 0.4300s\n",
            "Epoch: 0148 loss_train: 0.6934 acc_train: 0.7857 loss_val: 0.6588 acc_val: 0.8267 time: 0.4316s\n",
            "Epoch: 0149 loss_train: 0.6478 acc_train: 0.7786 loss_val: 0.6620 acc_val: 0.8300 time: 0.4304s\n",
            "Epoch: 0150 loss_train: 0.5790 acc_train: 0.8214 loss_val: 0.6648 acc_val: 0.8300 time: 0.4278s\n",
            "Epoch: 0151 loss_train: 0.6896 acc_train: 0.8071 loss_val: 0.6681 acc_val: 0.8267 time: 0.4273s\n",
            "Epoch: 0152 loss_train: 0.6170 acc_train: 0.8286 loss_val: 0.6650 acc_val: 0.8300 time: 0.4311s\n",
            "Epoch: 0153 loss_train: 0.7724 acc_train: 0.7571 loss_val: 0.6623 acc_val: 0.8267 time: 0.4332s\n",
            "Epoch: 0154 loss_train: 0.6711 acc_train: 0.7857 loss_val: 0.6575 acc_val: 0.8267 time: 0.4298s\n",
            "Epoch: 0155 loss_train: 0.5961 acc_train: 0.8714 loss_val: 0.6528 acc_val: 0.8333 time: 0.4373s\n",
            "Epoch: 0156 loss_train: 0.6236 acc_train: 0.7786 loss_val: 0.6484 acc_val: 0.8367 time: 0.4261s\n",
            "Epoch: 0157 loss_train: 0.5280 acc_train: 0.8429 loss_val: 0.6427 acc_val: 0.8367 time: 0.4394s\n",
            "Epoch: 0158 loss_train: 0.6647 acc_train: 0.7857 loss_val: 0.6386 acc_val: 0.8300 time: 0.4310s\n",
            "Epoch: 0159 loss_train: 0.7716 acc_train: 0.7071 loss_val: 0.6376 acc_val: 0.8333 time: 0.4303s\n",
            "Epoch: 0160 loss_train: 0.5979 acc_train: 0.8071 loss_val: 0.6376 acc_val: 0.8367 time: 0.4297s\n",
            "Epoch: 0161 loss_train: 0.6213 acc_train: 0.7929 loss_val: 0.6403 acc_val: 0.8400 time: 0.4309s\n",
            "Epoch: 0162 loss_train: 0.6303 acc_train: 0.8071 loss_val: 0.6442 acc_val: 0.8433 time: 0.4361s\n",
            "Epoch: 0163 loss_train: 0.7120 acc_train: 0.7643 loss_val: 0.6492 acc_val: 0.8400 time: 0.4281s\n",
            "Epoch: 0164 loss_train: 0.5569 acc_train: 0.8214 loss_val: 0.6526 acc_val: 0.8333 time: 0.4301s\n",
            "Epoch: 0165 loss_train: 0.7003 acc_train: 0.7714 loss_val: 0.6594 acc_val: 0.8267 time: 0.4272s\n",
            "Epoch: 0166 loss_train: 0.5763 acc_train: 0.8214 loss_val: 0.6694 acc_val: 0.8133 time: 0.4280s\n",
            "Epoch: 0167 loss_train: 0.7063 acc_train: 0.7571 loss_val: 0.6794 acc_val: 0.8067 time: 0.4329s\n",
            "Epoch: 0168 loss_train: 0.6167 acc_train: 0.8286 loss_val: 0.6813 acc_val: 0.8033 time: 0.4256s\n",
            "Epoch: 0169 loss_train: 0.5789 acc_train: 0.8143 loss_val: 0.6876 acc_val: 0.8067 time: 0.4302s\n",
            "Epoch: 0170 loss_train: 0.6949 acc_train: 0.7786 loss_val: 0.6944 acc_val: 0.7967 time: 0.4278s\n",
            "Epoch: 0171 loss_train: 0.6606 acc_train: 0.8071 loss_val: 0.7037 acc_val: 0.7933 time: 0.4290s\n",
            "Epoch: 0172 loss_train: 0.7171 acc_train: 0.8071 loss_val: 0.7072 acc_val: 0.7933 time: 0.4268s\n",
            "Epoch: 0173 loss_train: 0.6367 acc_train: 0.8357 loss_val: 0.7022 acc_val: 0.7967 time: 0.4297s\n",
            "Epoch: 0174 loss_train: 0.7528 acc_train: 0.7714 loss_val: 0.6989 acc_val: 0.7967 time: 0.4279s\n",
            "Epoch: 0175 loss_train: 0.6227 acc_train: 0.8571 loss_val: 0.6935 acc_val: 0.7967 time: 0.4271s\n",
            "Epoch: 0176 loss_train: 0.7083 acc_train: 0.7929 loss_val: 0.6891 acc_val: 0.8100 time: 0.4347s\n",
            "Epoch: 0177 loss_train: 0.5757 acc_train: 0.8214 loss_val: 0.6855 acc_val: 0.8167 time: 0.4348s\n",
            "Epoch: 0178 loss_train: 0.6981 acc_train: 0.7714 loss_val: 0.6810 acc_val: 0.8200 time: 0.4279s\n",
            "Epoch: 0179 loss_train: 0.5641 acc_train: 0.8643 loss_val: 0.6763 acc_val: 0.8267 time: 0.4257s\n",
            "Epoch: 0180 loss_train: 0.6678 acc_train: 0.7929 loss_val: 0.6707 acc_val: 0.8267 time: 0.4270s\n",
            "Epoch: 0181 loss_train: 0.6481 acc_train: 0.7857 loss_val: 0.6677 acc_val: 0.8333 time: 0.4313s\n",
            "Epoch: 0182 loss_train: 0.6184 acc_train: 0.7929 loss_val: 0.6661 acc_val: 0.8267 time: 0.4315s\n",
            "Epoch: 0183 loss_train: 0.6108 acc_train: 0.8429 loss_val: 0.6658 acc_val: 0.8167 time: 0.4316s\n",
            "Epoch: 0184 loss_train: 0.7374 acc_train: 0.7643 loss_val: 0.6672 acc_val: 0.8133 time: 0.4278s\n",
            "Epoch: 0185 loss_train: 0.6187 acc_train: 0.7786 loss_val: 0.6671 acc_val: 0.8133 time: 0.4295s\n",
            "Epoch: 0186 loss_train: 0.7369 acc_train: 0.7929 loss_val: 0.6656 acc_val: 0.8067 time: 0.4272s\n",
            "Epoch: 0187 loss_train: 0.6129 acc_train: 0.8143 loss_val: 0.6623 acc_val: 0.8033 time: 0.4289s\n",
            "Epoch: 0188 loss_train: 0.5834 acc_train: 0.8143 loss_val: 0.6586 acc_val: 0.8100 time: 0.4395s\n",
            "Epoch: 0189 loss_train: 0.6601 acc_train: 0.7571 loss_val: 0.6575 acc_val: 0.8133 time: 0.4286s\n",
            "Epoch: 0190 loss_train: 0.6834 acc_train: 0.7929 loss_val: 0.6573 acc_val: 0.8167 time: 0.4285s\n",
            "Epoch: 0191 loss_train: 0.6438 acc_train: 0.8071 loss_val: 0.6572 acc_val: 0.8000 time: 0.4317s\n",
            "Epoch: 0192 loss_train: 0.7327 acc_train: 0.7286 loss_val: 0.6541 acc_val: 0.8033 time: 0.4292s\n",
            "Epoch: 0193 loss_train: 0.7895 acc_train: 0.7500 loss_val: 0.6492 acc_val: 0.7933 time: 0.4271s\n",
            "Epoch: 0194 loss_train: 0.6642 acc_train: 0.8214 loss_val: 0.6468 acc_val: 0.8000 time: 0.4311s\n",
            "Epoch: 0195 loss_train: 0.6574 acc_train: 0.8000 loss_val: 0.6421 acc_val: 0.8033 time: 0.4306s\n",
            "Epoch: 0196 loss_train: 0.7936 acc_train: 0.7357 loss_val: 0.6394 acc_val: 0.8100 time: 0.4276s\n",
            "Epoch: 0197 loss_train: 0.7850 acc_train: 0.7357 loss_val: 0.6418 acc_val: 0.8167 time: 0.4297s\n",
            "Epoch: 0198 loss_train: 0.5299 acc_train: 0.8357 loss_val: 0.6415 acc_val: 0.8233 time: 0.4262s\n",
            "Epoch: 0199 loss_train: 0.5589 acc_train: 0.8286 loss_val: 0.6428 acc_val: 0.8200 time: 0.4267s\n",
            "Epoch: 0200 loss_train: 0.6665 acc_train: 0.8286 loss_val: 0.6464 acc_val: 0.8200 time: 0.4245s\n",
            "Epoch: 0201 loss_train: 0.6797 acc_train: 0.7786 loss_val: 0.6490 acc_val: 0.8167 time: 0.4313s\n",
            "Epoch: 0202 loss_train: 0.5761 acc_train: 0.8429 loss_val: 0.6474 acc_val: 0.8167 time: 0.4311s\n",
            "Epoch: 0203 loss_train: 0.5762 acc_train: 0.7929 loss_val: 0.6491 acc_val: 0.8200 time: 0.4274s\n",
            "Epoch: 0204 loss_train: 0.5466 acc_train: 0.8571 loss_val: 0.6506 acc_val: 0.8233 time: 0.4309s\n",
            "Epoch: 0205 loss_train: 0.7075 acc_train: 0.8000 loss_val: 0.6528 acc_val: 0.8200 time: 0.4325s\n",
            "Epoch: 0206 loss_train: 0.6052 acc_train: 0.8429 loss_val: 0.6533 acc_val: 0.8167 time: 0.4287s\n",
            "Epoch: 0207 loss_train: 0.6620 acc_train: 0.7857 loss_val: 0.6544 acc_val: 0.8100 time: 0.4266s\n",
            "Epoch: 0208 loss_train: 0.7301 acc_train: 0.7500 loss_val: 0.6547 acc_val: 0.8067 time: 0.4334s\n",
            "Epoch: 0209 loss_train: 0.6706 acc_train: 0.7857 loss_val: 0.6527 acc_val: 0.8100 time: 0.4302s\n",
            "Epoch: 0210 loss_train: 0.6425 acc_train: 0.7929 loss_val: 0.6525 acc_val: 0.8100 time: 0.4295s\n",
            "Epoch: 0211 loss_train: 0.5527 acc_train: 0.8429 loss_val: 0.6502 acc_val: 0.8033 time: 0.4280s\n",
            "Epoch: 0212 loss_train: 0.7111 acc_train: 0.7857 loss_val: 0.6508 acc_val: 0.8067 time: 0.4262s\n",
            "Epoch: 0213 loss_train: 0.6563 acc_train: 0.8214 loss_val: 0.6515 acc_val: 0.8033 time: 0.4277s\n",
            "Epoch: 0214 loss_train: 0.6234 acc_train: 0.7714 loss_val: 0.6532 acc_val: 0.8033 time: 0.4267s\n",
            "Epoch: 0215 loss_train: 0.7163 acc_train: 0.7571 loss_val: 0.6565 acc_val: 0.8100 time: 0.4308s\n",
            "Epoch: 0216 loss_train: 0.6949 acc_train: 0.7500 loss_val: 0.6590 acc_val: 0.8133 time: 0.4330s\n",
            "Epoch: 0217 loss_train: 0.6986 acc_train: 0.7714 loss_val: 0.6621 acc_val: 0.8200 time: 0.4294s\n",
            "Epoch: 0218 loss_train: 0.5944 acc_train: 0.8357 loss_val: 0.6638 acc_val: 0.8267 time: 0.4322s\n",
            "Epoch: 0219 loss_train: 0.6168 acc_train: 0.8143 loss_val: 0.6636 acc_val: 0.8267 time: 0.4291s\n",
            "Epoch: 0220 loss_train: 0.6141 acc_train: 0.8000 loss_val: 0.6619 acc_val: 0.8300 time: 0.4260s\n",
            "Epoch: 0221 loss_train: 0.6581 acc_train: 0.8000 loss_val: 0.6572 acc_val: 0.8333 time: 0.4315s\n",
            "Epoch: 0222 loss_train: 0.5865 acc_train: 0.8357 loss_val: 0.6520 acc_val: 0.8367 time: 0.4292s\n",
            "Epoch: 0223 loss_train: 0.6112 acc_train: 0.8143 loss_val: 0.6465 acc_val: 0.8333 time: 0.4312s\n",
            "Epoch: 0224 loss_train: 0.6401 acc_train: 0.8286 loss_val: 0.6432 acc_val: 0.8333 time: 0.4294s\n",
            "Epoch: 0225 loss_train: 0.5769 acc_train: 0.8429 loss_val: 0.6421 acc_val: 0.8233 time: 0.4313s\n",
            "Epoch: 0226 loss_train: 0.6155 acc_train: 0.8000 loss_val: 0.6426 acc_val: 0.8233 time: 0.4273s\n",
            "Epoch: 0227 loss_train: 0.7090 acc_train: 0.7786 loss_val: 0.6434 acc_val: 0.8200 time: 0.4311s\n",
            "Epoch: 0228 loss_train: 0.6562 acc_train: 0.7786 loss_val: 0.6441 acc_val: 0.8200 time: 0.4276s\n",
            "Epoch: 0229 loss_train: 0.6833 acc_train: 0.7643 loss_val: 0.6438 acc_val: 0.8200 time: 0.4285s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKz4dSHXyGkt"
      },
      "source": [
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        os.remove(file)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qahzd5QIyaPN",
        "outputId": "a09ede64-a2d0-4c9e-b399-6798b3106009"
      },
      "source": [
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "print('Loading {}th epoch'.format(best_epoch))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 99.1086s\n",
            "Loading 128th epoch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "za8WoUeY6PAT",
        "outputId": "1a8ff850-7ec0-4dc1-c328-e1f108bd71ae"
      },
      "source": [
        "compute_test()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-ab59079914ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-0b1202e7896a>\u001b[0m in \u001b[0;36mcompute_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   print(\"Test set results:\",\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;34m\"loss= {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \"accuracy= {:.4f}\".format(acc_test.data[0]))\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
          ]
        }
      ]
    }
  ]
}